{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to the NLTK\n",
    "\n",
    "This file introduces the reader to the Python Natural Langauge Toolkit (NLTK).\n",
    "\n",
    "The Python NLTK is a set of modules and corpora enabling the reader to do natural langauge processing against corpora of one or more texts. It goes beyond text minnig and provides tools to do machine learning, but this Notebook barely scratches that surface.\n",
    "\n",
    "This is my first Python Jupyter Notebook. As such I'm sure there will be errors in implementation, style, and functionality. For example, the Notebook may fail because the value of FILE is too operating system dependent, or the given file does not exist. Other failures may/will include the lack of additional modules. In these cases, simply read the error messages and follow the instructions. \"Your mileage may vary.\"\n",
    "\n",
    "That said, through the use of this Notebook, the reader ought to be able to get a flavor for what the Toolkit can do without the need to completly understand the Python language.\n",
    "\n",
    "--  \n",
    "Eric Lease Morgan _<emorgan@nd.edu_>  \n",
    "April 12, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure; using an absolute path, define the location of a plain text file for analysis\n",
    "FILE = 'walden.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import / require the use of the Toolkit\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slurp up the given file; display the result\n",
    "handle = open( FILE, 'r')\n",
    "data   = handle.read()\n",
    "print( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data into features (words); display them\n",
    "features = word_tokenize( data )\n",
    "print( features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features to lower case and exclude punctuation\n",
    "features = [ feature for feature in features if feature.isalpha() ]\n",
    "features = [ feature.lower() for feature in features ]\n",
    "print( features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of (English) stopwords, and then remove them from the features\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words( 'english' )\n",
    "features  = [ feature for feature in features if feature not in stopwords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count & tabulate the features, and then plot the results -- season to taste\n",
    "frequencies = FreqDist( features )\n",
    "plot = frequencies.plot( 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words (hapaxes); display them\n",
    "hapaxes = frequencies.hapaxes()\n",
    "print( hapaxes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count & tabulate ngrams from the features -- season to taste; display some\n",
    "ngrams      = ngrams( features, 2 )\n",
    "frequencies = FreqDist( ngrams )\n",
    "frequencies.most_common( 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list each token's length, and plot the result; How many \"long\" words are there?\n",
    "lengths = [ len( feature ) for feature in features ]\n",
    "plot    = FreqDist( lengths ).plot( 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a stemmer, stem the features, count & tabulate, and output\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer     = PorterStemmer()\n",
    "stems       = [ stemmer.stem( feature ) for feature in features ]\n",
    "frequencies = FreqDist( stems )\n",
    "frequencies.most_common( 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create the features and create a NLTK Text object, so other cool things can be done\n",
    "features = word_tokenize( data )\n",
    "text     = Text( features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count & tabulate, again; list a given word -- season to taste\n",
    "frequencies = FreqDist( text )\n",
    "print( frequencies[ 'love' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do keyword-in-context searching against the text (concordancing)\n",
    "print( text.concordance( 'love' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dispersion plot of given words\n",
    "plot = text.dispersion_plot( [ 'love', 'war', 'man', 'god' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the \"most significant\" bigrams, considering surrounding words (size of window) -- season to taste\n",
    "text.collocations( num=10, window_size=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a set of words, what words are nearby\n",
    "text.common_contexts( [ 'love', 'war', 'man', 'god' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the words (features) most associated with the given word\n",
    "text.similar( 'love' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of sentences, and display one -- season to taste\n",
    "sentences = sent_tokenize( data )\n",
    "sentence  = sentences[ 14 ]\n",
    "print( sentence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentence and parse it into parts-of-speech, all in one go\n",
    "sentence = pos_tag( word_tokenize( sentence ) )\n",
    "print( sentence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract named enities from a sentence, and print the results\n",
    "entities = ne_chunk( sentence )\n",
    "print( entities )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the entites graphically\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the Notebook. I hope you found it useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
